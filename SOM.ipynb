{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6HE59u21zTIk",
        "9aKLEdkJybYY",
        "9fjgBClYqe3_",
        "ASSJ_UW80c4P",
        "ZhC_hROXtd7w",
        "YUp4My6JvezM",
        "E5SLZJdjz9Xi",
        "BOHsSjPUAvmU",
        "bA4o_Y2wqmfT",
        "-9TGxfebS4MX",
        "LuhmAyOyriAz",
        "Izaw5vm7N6EW",
        "ZNYi3C9Baqhy"
      ],
      "authorship_tag": "ABX9TyPz3VuuhDYrkt+fz96WCVgq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fredfurst/som/blob/master/SOM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "887M5QqdtkB5"
      },
      "source": [
        "Taking from https://github.com/lyes-khacef/GPU-SOM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Jq-pQyzU2M"
      },
      "source": [
        "Now mixing it up with data from a notebook in the ChaosDoc folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HE59u21zTIk"
      },
      "source": [
        "## 1. Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQCjVQQKzFML"
      },
      "source": [
        "### GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRC7-Z8qsbW9",
        "outputId": "1ee19d8a-a966-418f-e36e-0c60887352d7"
      },
      "source": [
        "!pip install GPUtil"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=f59cde4206037c4333c3d3e672bb5f573bb1de3d8aae7b8ee4ade2cac06dbef8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOp27HwhuPxD"
      },
      "source": [
        "### From imports.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgBbUj5xsn7s",
        "outputId": "42167ba7-0470-4045-85bf-7e59d51b7afa"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import timeit\n",
        "\n",
        "# import packages\n",
        "import os, sys, humanize, psutil, GPUtil\n",
        "\n",
        "# diable gpu\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "\n",
        "# disable tensorflow logs\n",
        "\"\"\"\n",
        "0 = all messages are logged (default behavior)\n",
        "1 = INFO messages are not printed\n",
        "2 = INFO and WARNING messages are not printed\n",
        "3 = INFO, WARNING, and ERROR messages are not printed\n",
        "\"\"\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
        "\n",
        "# import tensorflow\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version = \", tf.__version__)\n",
        "if tf.executing_eagerly():\n",
        "    print(\"Eager execution!\")\n",
        "#tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version =  2.15.0\n",
            "Eager execution!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s29VW9OQuKLA"
      },
      "source": [
        "### From gpu_check.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xA9Nh8YtqtH"
      },
      "source": [
        "Check for GPUs..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCsHSAelsti6",
        "outputId": "7d2c0bf3-3af6-4ea7-db28-54e15fb0c814"
      },
      "source": [
        "# imports\n",
        "#from imports import *\n",
        "\n",
        "# define function\n",
        "def gpu_report():\n",
        "    print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
        "    GPUs = GPUtil.getGPUs()\n",
        "    print(GPUs)\n",
        "    for i, gpu in enumerate(GPUs):\n",
        "        print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU RAM Free: 12.4 GB\n",
            "[<GPUtil.GPUtil.GPU object at 0x7d4df3d97010>]\n",
            "GPU 0 ... Mem Free: 15101MB / 15360MB | Utilization   0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aKLEdkJybYY"
      },
      "source": [
        "## 2. Uploading file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "osv1DBEbyUSX",
        "outputId": "21614e6c-d182-4d4a-e0ad-8c0a365add30"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded_file = files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c2e686df-7283-4cd7-8276-4f7f2a773931\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c2e686df-7283-4cd7-8276-4f7f2a773931\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-71e133504b20>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    157\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    158\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlrLHp6tBMUL"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1a3bz1m6wMe"
      },
      "source": [
        "uploaded_file.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt0rj9Zlzg2l"
      },
      "source": [
        "df1 = pd.read_fwf(\n",
        "    io.BytesIO(uploaded_file['e_composite_im7_8552_OHT_b-OHT-Hard-J3-ExpRed_S11.csv']),\n",
        "    header=[1,2,3],\n",
        "    skip_blank_lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJMupLwW1vFu"
      },
      "source": [
        "df1.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW2rV--hbJik"
      },
      "source": [
        "### Get Column names For further vizes..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqz-gOW41xfN"
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-NHF0OV4TXr"
      },
      "source": [
        "npdf1 = np.array(df1[:400],dtype=float).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrJxigAO8fMK"
      },
      "source": [
        "npdf1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fjgBClYqe3_"
      },
      "source": [
        "# 3. Boilerplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASSJ_UW80c4P"
      },
      "source": [
        "### From data_load.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70uOr3qV0eDz"
      },
      "source": [
        "# imports\n",
        "#from imports import *\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# fuction to load data from file\n",
        "def load_data(name):\n",
        "    data = np.load(name, allow_pickle=True)\n",
        "    print(\"Data.shape = \", data.shape)\n",
        "    return data\n",
        "\n",
        "def get_dataset(train_data, label_data, test_data):\n",
        "    # importing dataset\n",
        "    (x_train_all, index_train_all), (x_test_all, index_test_all) = mnist.load_data()\n",
        "    x_train_all = x_train_all.astype('float32') / 255.\n",
        "    x_test_all = x_test_all.astype('float32') / 255.\n",
        "    x_train_all = x_train_all.reshape((60000, 784))\n",
        "    x_test_all = x_test_all.reshape((10000, 784))\n",
        "\n",
        "    # constructing dataset\n",
        "    x_train = np.copy(x_train_all[:train_data,:])\n",
        "    index_train = np.copy(index_train_all[:train_data])\n",
        "    x_label = np.copy(x_train_all[:label_data,:])\n",
        "    index_label = np.copy(index_train_all[:label_data])\n",
        "    x_test = np.copy(x_test_all[:test_data,:])\n",
        "    index_test = np.copy(index_test_all[:test_data])\n",
        "\n",
        "    return x_train, index_train, x_label, index_label, x_test, index_test, label_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1TzQRWSzmEF"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhC_hROXtd7w"
      },
      "source": [
        "### From file tf_ksom.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGgYl9QT9_gv"
      },
      "source": [
        "class KSOM_3D():\n",
        "    def __init__(self, m, n, o, dim):\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.o = o\n",
        "        self.dim = dim\n",
        "        self.map_wgt =  tf.Variable(\n",
        "                            tf.random.uniform(\n",
        "                                shape = [m*n*o, dim],\n",
        "                                minval = 0.0,\n",
        "                                maxval = 1.0,\n",
        "                                dtype = tf.float32,\n",
        "                                seed = 23\n",
        "                            )\n",
        "                        )\n",
        "        self.map_loc =  tf.constant(\n",
        "                            np.array(\n",
        "                                list(self.neuron_locs(m, n, o))\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "    def neuron_locs(self, m, n, o):\n",
        "        # nested iterations over both dimensions to yield one by one the 2-d locations of the individual neurons in the SOM\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                for k in range(o):\n",
        "                    yield np.array([i,j,k], dtype=np.float32)\n",
        "\n",
        "    def compute_winner(self, sample):\n",
        "            self.sample = sample\n",
        "\n",
        "            # compute the squared euclidean distance between the input and the neurons\n",
        "            self.squared_distance = tf.reduce_sum(\n",
        "                                        tf.square(\n",
        "                                            tf.subtract(\n",
        "                                                self.map_wgt, # [m*n*o, dim]\n",
        "                                                tf.expand_dims(\n",
        "                                                    self.sample, # [dim] -> [1, dim]\n",
        "                                                    axis=0\n",
        "                                                )\n",
        "                                            )\n",
        "                                        ),\n",
        "                                        axis=1\n",
        "                                    )\n",
        "\n",
        "            # find the bmu's index\n",
        "            self.bmu_idx =  tf.argmin(\n",
        "                                    input=self.squared_distance,\n",
        "                                    axis=0\n",
        "                                )\n",
        "\n",
        "            # extract the bmu's 2-d location\n",
        "            self.bmu_loc =  tf.gather(\n",
        "                                self.map_loc,\n",
        "                                self.bmu_idx\n",
        "                            )\n",
        "\n",
        "    def update_network(self, epsilon, eta):\n",
        "        # compute the squared manhattan distance between the bmu and the neurons\n",
        "        self.bmu_distance_squares = tf.reduce_sum(\n",
        "                                        tf.square(\n",
        "                                            tf.subtract(\n",
        "                                                self.map_loc, # [m*n*o, 2]\n",
        "                                                tf.expand_dims(\n",
        "                                                    self.bmu_loc, # [2] -> [1, 2]\n",
        "                                                    axis=0\n",
        "                                                )\n",
        "                                            )\n",
        "                                        ),\n",
        "                                        axis=1\n",
        "                                    )\n",
        "\n",
        "        # compute the neighborhood function\n",
        "        self.neighbourhood_func = tf.exp(\n",
        "                                      tf.negative(\n",
        "                                          tf.math.divide(\n",
        "                                              self.bmu_distance_squares,\n",
        "                                              tf.multiply(\n",
        "                                                  tf.square(\n",
        "                                                      eta,\n",
        "                                                  ),\n",
        "                                                  2.0\n",
        "                                              )\n",
        "                                          )\n",
        "                                      )\n",
        "                                  )\n",
        "\n",
        "        # compute the overall learning of each neuron\n",
        "        self.learning = tf.multiply(\n",
        "                            self.neighbourhood_func,\n",
        "                            epsilon\n",
        "                        )\n",
        "\n",
        "        # compute the difference between the neurons weights and the input\n",
        "        self.delta_wgt =  tf.subtract(\n",
        "                              tf.expand_dims(\n",
        "                                  self.sample, # [dim] -> [1, dim]\n",
        "                                  axis=0\n",
        "                              ),\n",
        "                              self.map_wgt, # [m*n*o, dim]\n",
        "                          )\n",
        "\n",
        "        # compute the weights update according to the learning and delta_wgt and update the weights\n",
        "        tf.compat.v1.assign_add(\n",
        "            self.map_wgt,\n",
        "            tf.multiply(\n",
        "                tf.expand_dims(\n",
        "                    self.learning, # [m*n*o] -> [m*n*o, 1]\n",
        "                    axis=-1\n",
        "                ),\n",
        "                self.delta_wgt # [m*n*o, dim]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.map_wgt\n",
        "\n",
        "    @tf.function\n",
        "    def train(self, nbr_epochs, epsilon_i, epsilon_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test):\n",
        "        with tf.device('/device:gpu:0'):\n",
        "            for epoch in tf.range(nbr_epochs):\n",
        "                tf.print(\"---------- Epoch\", epoch + 1, \"----------\")\n",
        "\n",
        "                # update the learning rate epsilon\n",
        "                epsilon_t =  tf.multiply(\n",
        "                                    epsilon_i,\n",
        "                                    tf.pow(\n",
        "                                        tf.math.divide(\n",
        "                                            epsilon_f,\n",
        "                                            epsilon_i\n",
        "                                        ),\n",
        "                                        tf.cast(\n",
        "                                            tf.math.divide(\n",
        "                                                epoch,\n",
        "                                                nbr_epochs - 1\n",
        "                                            ),\n",
        "                                            dtype=tf.float32\n",
        "                                        )\n",
        "                                    )\n",
        "                                )\n",
        "\n",
        "                # update the gaussian neighborhood witdh eta\n",
        "                eta_t =  tf.multiply(\n",
        "                                  eta_i,\n",
        "                                  tf.pow(\n",
        "                                      tf.math.divide(\n",
        "                                          eta_f,\n",
        "                                          eta_i\n",
        "                                      ),\n",
        "                                      tf.cast(\n",
        "                                          tf.math.divide(\n",
        "                                              epoch,\n",
        "                                              nbr_epochs - 1\n",
        "                                          ),\n",
        "                                          dtype=tf.float32\n",
        "                                      )\n",
        "                                  )\n",
        "                              )\n",
        "\n",
        "                # shuffle the training dataset\n",
        "                tf.random.shuffle(x_train)\n",
        "\n",
        "                # bmu computing and network update for each sample\n",
        "                for x_trn in x_train:\n",
        "                    sample = tf.cast(x_trn, dtype=tf.float32)\n",
        "                    self.compute_winner(sample)\n",
        "                    self.update_network(epsilon_t, eta_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9xYsJzJsxVs"
      },
      "source": [
        "# imports\n",
        "#from imports import *\n",
        "\n",
        "class KSOM():\n",
        "    def __init__(self, m, n, dim):\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.dim = dim\n",
        "        self.map_wgt =  tf.Variable(\n",
        "                            tf.random.uniform(\n",
        "                                shape = [m*n, dim],\n",
        "                                minval = 0.0,\n",
        "                                maxval = 1.0,\n",
        "                                dtype = tf.float32,\n",
        "                                seed = 23\n",
        "                            )\n",
        "                        )\n",
        "        self.map_loc =  tf.constant(\n",
        "                            np.array(\n",
        "                                list(self.neuron_locs(m, n))\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "    def neuron_locs(self, m, n):\n",
        "        # nested iterations over both dimensions to yield one by one the 2-d locations of the individual neurons in the SOM\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield np.array([i,j], dtype=np.float32)\n",
        "\n",
        "    def compute_winner(self, sample):\n",
        "            self.sample = sample\n",
        "\n",
        "            # compute the squared euclidean distance between the input and the neurons\n",
        "            self.squared_distance = tf.reduce_sum(\n",
        "                                        tf.square(\n",
        "                                            tf.subtract(\n",
        "                                                self.map_wgt, # [m*n, dim]\n",
        "                                                tf.expand_dims(\n",
        "                                                    self.sample, # [dim] -> [1, dim]\n",
        "                                                    axis=0\n",
        "                                                )\n",
        "                                            )\n",
        "                                        ),\n",
        "                                        axis=1\n",
        "                                    )\n",
        "\n",
        "            # find the bmu's index\n",
        "            self.bmu_idx =  tf.argmin(\n",
        "                                    input=self.squared_distance,\n",
        "                                    axis=0\n",
        "                                )\n",
        "\n",
        "            # extract the bmu's 2-d location\n",
        "            self.bmu_loc =  tf.gather(\n",
        "                                self.map_loc,\n",
        "                                self.bmu_idx\n",
        "                            )\n",
        "\n",
        "    def update_network(self, epsilon, eta):\n",
        "        # compute the squared manhattan distance between the bmu and the neurons\n",
        "        self.bmu_distance_squares = tf.reduce_sum(\n",
        "                                        tf.square(\n",
        "                                            tf.subtract(\n",
        "                                                self.map_loc, # [m*n, 2]\n",
        "                                                tf.expand_dims(\n",
        "                                                    self.bmu_loc, # [2] -> [1, 2]\n",
        "                                                    axis=0\n",
        "                                                )\n",
        "                                            )\n",
        "                                        ),\n",
        "                                        axis=1\n",
        "                                    )\n",
        "\n",
        "        # compute the neighborhood function\n",
        "        self.neighbourhood_func = tf.exp(\n",
        "                                      tf.negative(\n",
        "                                          tf.math.divide(\n",
        "                                              self.bmu_distance_squares,\n",
        "                                              tf.multiply(\n",
        "                                                  tf.square(\n",
        "                                                      eta,\n",
        "                                                  ),\n",
        "                                                  2.0\n",
        "                                              )\n",
        "                                          )\n",
        "                                      )\n",
        "                                  )\n",
        "\n",
        "        # compute the overall learning of each neuron\n",
        "        self.learning = tf.multiply(\n",
        "                            self.neighbourhood_func,\n",
        "                            epsilon\n",
        "                        )\n",
        "\n",
        "        # compute the difference between the neurons weights and the input\n",
        "        self.delta_wgt =  tf.subtract(\n",
        "                              tf.expand_dims(\n",
        "                                  self.sample, # [dim] -> [1, dim]\n",
        "                                  axis=0\n",
        "                              ),\n",
        "                              self.map_wgt, # [m*n, dim]\n",
        "                          )\n",
        "\n",
        "        # compute the weights update according to the learning and delta_wgt and update the weights\n",
        "        tf.compat.v1.assign_add(\n",
        "            self.map_wgt,\n",
        "            tf.multiply(\n",
        "                tf.expand_dims(\n",
        "                    self.learning, # [m*n] -> [m*n, 1]\n",
        "                    axis=-1\n",
        "                ),\n",
        "                self.delta_wgt # [m*n, dim]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.map_wgt\n",
        "\n",
        "    @tf.function\n",
        "    def train(self, nbr_epochs, epsilon_i, epsilon_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test):\n",
        "        with tf.device('/device:gpu:0'):\n",
        "            for epoch in tf.range(nbr_epochs):\n",
        "                tf.print(\"---------- Epoch\", epoch + 1, \"----------\")\n",
        "\n",
        "                # update the learning rate epsilon\n",
        "                epsilon_t =  tf.multiply(\n",
        "                                    epsilon_i,\n",
        "                                    tf.pow(\n",
        "                                        tf.math.divide(\n",
        "                                            epsilon_f,\n",
        "                                            epsilon_i\n",
        "                                        ),\n",
        "                                        tf.cast(\n",
        "                                            tf.math.divide(\n",
        "                                                epoch,\n",
        "                                                nbr_epochs - 1\n",
        "                                            ),\n",
        "                                            dtype=tf.float32\n",
        "                                        )\n",
        "                                    )\n",
        "                                )\n",
        "\n",
        "                # update the gaussian neighborhood witdh eta\n",
        "                eta_t =  tf.multiply(\n",
        "                                  eta_i,\n",
        "                                  tf.pow(\n",
        "                                      tf.math.divide(\n",
        "                                          eta_f,\n",
        "                                          eta_i\n",
        "                                      ),\n",
        "                                      tf.cast(\n",
        "                                          tf.math.divide(\n",
        "                                              epoch,\n",
        "                                              nbr_epochs - 1\n",
        "                                          ),\n",
        "                                          dtype=tf.float32\n",
        "                                      )\n",
        "                                  )\n",
        "                              )\n",
        "\n",
        "                # shuffle the training dataset\n",
        "                tf.random.shuffle(x_train)\n",
        "\n",
        "                # bmu computing and network update for each sample\n",
        "                for x_trn in x_train:\n",
        "                    sample = tf.cast(x_trn, dtype=tf.float32)\n",
        "                    self.compute_winner(sample)\n",
        "                    self.update_network(epsilon_t, eta_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUp4My6JvezM"
      },
      "source": [
        "### From label.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en6uoQRUveCP"
      },
      "source": [
        "# imports\n",
        "#from imports import *\n",
        "from test import *\n",
        "\n",
        "def labeling(label_data, class_nbr, weights, x_label, index_label, sigma):\n",
        "    dist_sum = np.zeros((len(weights), class_nbr))\n",
        "    nbr_digits = np.zeros((class_nbr,))\n",
        "\n",
        "    # accumulate the normalized gaussian distance for the labeling dataset\n",
        "    for (x, y) in zip(x_label, index_label):\n",
        "        nbr_digits[y] += 1\n",
        "        dist_neuron = np.exp(-np.linalg.norm(x - weights, axis=1)/sigma)\n",
        "        dist_bmu = np.max(dist_neuron)\n",
        "        for i, distn in enumerate(dist_neuron):\n",
        "            dist_sum[i][y] += distn/dist_bmu\n",
        "\n",
        "    # normalize the activities on the number of samples per class\n",
        "    for i, dists in enumerate(dist_sum):\n",
        "        dist_sum[i] = dists/nbr_digits\n",
        "\n",
        "    # assign the neurons labels\n",
        "    neuron_label = np.argmax(dist_sum, axis=1)\n",
        "    print(\"Neurons labels = \")\n",
        "    print(neuron_label)\n",
        "\n",
        "    return neuron_label\n",
        "\n",
        "def unison_shuffle(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "def run_labeling(weights, label_data, x_tr, index_tr, x_ts, index_ts):\n",
        "    class_nbr = 10\n",
        "    sigma_kernel = 1.0\n",
        "    for i in range(10):\n",
        "        x_tr, index_tr = unison_shuffle(x_tr, index_tr)\n",
        "        x_lb = np.copy(x_tr[:label_data,:])\n",
        "        index_lb = np.copy(index_tr[:label_data])\n",
        "\n",
        "        # label the network\n",
        "        neuron_label = labeling(label_data, class_nbr, weights, x_lb, index_lb, sigma_kernel)\n",
        "\n",
        "        # test the network\n",
        "        accuracy = test(class_nbr, weights, x_ts, index_ts, neuron_label, sigma_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX3nufEBt-xM"
      },
      "source": [
        "### From test.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asaKcZRStV4v"
      },
      "source": [
        "# from imports import *\n",
        "\n",
        "def test(class_nbr, weights, x_test, index_test, neuron_label, sigma):\n",
        "    neuron_index = np.zeros((len(x_test),), dtype=int)\n",
        "\n",
        "    # calculate the BMUs for the test dataset\n",
        "    for i, x in enumerate(x_test):\n",
        "        dist_neuron = np.linalg.norm(x - weights, axis=1)\n",
        "        neuron_index[i] = np.argmin(dist_neuron)\n",
        "\n",
        "    # compare the BMUs labels and the samples labels\n",
        "    accuracy = 0\n",
        "    for p, t in zip(neuron_index, index_test):\n",
        "        if neuron_label[p] == t:\n",
        "            accuracy += 1\n",
        "    accuracy = (float(accuracy)/len(x_test))*100\n",
        "    print(\"SOM test accuracy = %.2f\\n\" % accuracy)\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5SLZJdjz9Xi"
      },
      "source": [
        "# Adjustments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSHRg9DtuibD"
      },
      "source": [
        "## From main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOHsSjPUAvmU"
      },
      "source": [
        "#### Later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5VwZcytoBuW"
      },
      "source": [
        "##### Mapa 20 por 20 = 400 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzCPcPWZuBfK"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 20 # 10\n",
        "map_hgt = 20 # 10\n",
        "map_dpt = 8\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM_3D(map_wth, map_hgt, map_dpt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAxHUdeg-xCG"
      },
      "source": [
        "# GPU memory check\n",
        "gpu_report()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc8YlV34BNZc"
      },
      "source": [
        "for k in range(map_dpt):\n",
        "  # display neurons weights as mnist digits\n",
        "  som_grid = plt.figure(figsize=(20, 20)) # width, height in inches\n",
        "  for n in range(map_wth*map_hgt):\n",
        "      ##\n",
        "      ## Must make this into time series plot. Shouldnt be the thing\n",
        "      ##\n",
        "      #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "      sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "      sub.set_axis_off()\n",
        "      clr = sub.plot(weights[n][k]) # imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "      #plt.colorbar(clr)\n",
        "  #plt.savefig(\"plots/som_weights.png\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf9RW9P6n21I"
      },
      "source": [
        "##### Mapa 10 por 10 = 100 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owpAMTdYToa3"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# imports\n",
        "#from imports import *\n",
        "#from gpu_check import *\n",
        "#from data_load import *\n",
        "#from tf_ksom import *\n",
        "#from label import *\n",
        "#from test import *\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 10\n",
        "map_hgt = 10\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM(map_wth, map_hgt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn7V0R8xToa7"
      },
      "source": [
        "## hyper parameters below\n",
        "## label list below\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()\n",
        "\n",
        "# display neurons weights as mnist digits\n",
        "som_grid = plt.figure(figsize=(200, 200)) # width, height in inches\n",
        "for n in range(map_wth*map_hgt):\n",
        "    ##\n",
        "    ## Must make this into time series plot. Shouldnt be the thing\n",
        "    ##\n",
        "    #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "    sub.set_axis_off()\n",
        "    clr = sub.plot(weights[n]) # imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "    #plt.colorbar(clr)\n",
        "#plt.savefig(\"plots/som_weights.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNJswyuzoctU"
      },
      "source": [
        "##### Mapa 5 por 5 = 25 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Ewk17JTt-q"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# imports\n",
        "#from imports import *\n",
        "#from gpu_check import *\n",
        "#from data_load import *\n",
        "#from tf_ksom import *\n",
        "#from label import *\n",
        "#from test import *\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 5\n",
        "map_hgt = 5\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM(map_wth, map_hgt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2PjlImfTt-r"
      },
      "source": [
        "## hyper parameters below\n",
        "## label list below\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()\n",
        "\n",
        "# display neurons weights as mnist digits\n",
        "som_grid = plt.figure(figsize=(200, 200)) # width, height in inches\n",
        "for n in range(map_wth*map_hgt):\n",
        "    ##\n",
        "    ## Must make this into time series plot. Shouldnt be the thing\n",
        "    ##\n",
        "    #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "    sub.set_axis_off()\n",
        "    clr = sub.plot(weights[n]) # imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "    #plt.colorbar(clr)\n",
        "#plt.savefig(\"plots/som_weights.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwqctnJ-p4Rs"
      },
      "source": [
        "##### Mapa 4 por 4 = 16 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E313I_kokU2C"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# imports\n",
        "#from imports import *\n",
        "#from gpu_check import *\n",
        "#from data_load import *\n",
        "#from tf_ksom import *\n",
        "#from label import *\n",
        "#from test import *\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 4\n",
        "map_hgt = 4\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM(map_wth, map_hgt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jepFqSPQkU2J"
      },
      "source": [
        "## hyper parameters below\n",
        "## label list below\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()\n",
        "\n",
        "# display neurons weights as mnist digits\n",
        "som_grid = plt.figure(figsize=(200, 200)) # width, height in inches\n",
        "for n in range(map_wth*map_hgt):\n",
        "    ##\n",
        "    ## Must make this into time series plot. Shouldnt be the thing\n",
        "    ##\n",
        "    #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "    sub.set_axis_off()\n",
        "    clr = sub.plot(weights[n]) # imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "    #plt.colorbar(clr)\n",
        "#plt.savefig(\"plots/som_weights.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA4o_Y2wqmfT"
      },
      "source": [
        "#### Mapa 3 por 3 = 9 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt2SzgvIdeIK"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# imports\n",
        "#from imports import *\n",
        "#from gpu_check import *\n",
        "#from data_load import *\n",
        "#from tf_ksom import *\n",
        "#from label import *\n",
        "#from test import *\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 3\n",
        "map_hgt = 3\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM(map_wth, map_hgt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2cLey3zTNNO"
      },
      "source": [
        "# display neurons weights as time series\n",
        "som_grid = plt.figure(figsize=(10, 10)) # width, height in inches\n",
        "for i in range(map_wth):\n",
        "  for j in range(map_hgt):\n",
        "    sub = som_grid.add_subplot(map_wth*map_dpt, map_hgt, 4*k+2*i+j+1)\n",
        "    for k in range(map_dpt):\n",
        "        #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "        sub.set_axis_off()\n",
        "        clr = sub.plot(range(400),weights[4*k+2*i+j],k)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9GuYC_IUpFe"
      },
      "source": [
        "print('i:'+str(i)+' and j: '+str(j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2rQ1dzOdeIM"
      },
      "source": [
        "## hyper parameters below\n",
        "## label list below\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()\n",
        "\n",
        "# display neurons weights as mnist digits\n",
        "som_grid = plt.figure(figsize=(200, 200)) # width, height in inches\n",
        "for n in range(map_wth*map_hgt*map_dpt):\n",
        "    ##\n",
        "    ## Must make this into time series plot. Shouldnt be the thing\n",
        "    ##\n",
        "    #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "    sub.set_axis_off()\n",
        "    clr = sub.plot(weights[n]) # imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "    #plt.colorbar(clr)\n",
        "#plt.savefig(\"plots/som_weights.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9TGxfebS4MX"
      },
      "source": [
        "#### Mapa 3 por 3 por 3 = 27 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARvROyA6S66O"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvaCQOBoS7VY"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# imports\n",
        "#from imports import *\n",
        "#from gpu_check import *\n",
        "#from data_load import *\n",
        "#from tf_ksom import *\n",
        "#from label import *\n",
        "#from test import *\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 3\n",
        "map_hgt = 3\n",
        "map_dpt = 3\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM_3D(map_wth, map_hgt, map_dpt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKtk3t3qS7VZ"
      },
      "source": [
        "# display neurons weights as time series\n",
        "som_grid = plt.figure(figsize=(10, 10)) # width, height in inches\n",
        "for i in range(map_wth):\n",
        "  for j in range(map_hgt):\n",
        "    for k in range(map_dpt):\n",
        "      sub = som_grid.add_subplot(map_wth, map_hgt, 3*j+i+1)\n",
        "      #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "      sub.set_axis_off()\n",
        "      clr = sub.plot(range(400),weights[9*k+3*j+i],9*k+3*j+i+1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9J1cgzOrNqX"
      },
      "source": [
        "##### Mapa 2 por 3 = 6 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU2ARENAd7if"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# imports\n",
        "#from imports import *\n",
        "#from gpu_check import *\n",
        "#from data_load import *\n",
        "#from tf_ksom import *\n",
        "#from label import *\n",
        "#from test import *\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 2\n",
        "map_hgt = 3\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM(map_wth, map_hgt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZfxx2ped7ih"
      },
      "source": [
        "## hyper parameters below\n",
        "## label list below\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()\n",
        "\n",
        "# display neurons weights as mnist digits\n",
        "som_grid = plt.figure(figsize=(200, 200)) # width, height in inches\n",
        "for n in range(map_wth*map_hgt):\n",
        "    ##\n",
        "    ## Must make this into time series plot. Shouldnt be the thing\n",
        "    ##\n",
        "    #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "    sub.set_axis_off()\n",
        "    clr = sub.plot(weights[n]) # imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "    #plt.colorbar(clr)\n",
        "#plt.savefig(\"plots/som_weights.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhmAyOyriAz"
      },
      "source": [
        "#### Mapa 2 por 2 = 4 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrW4r5_wd8Om"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# imports\n",
        "#from imports import *\n",
        "#from gpu_check import *\n",
        "#from data_load import *\n",
        "#from tf_ksom import *\n",
        "#from label import *\n",
        "#from test import *\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 2\n",
        "map_hgt = 2\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## make the ref from outside scope\n",
        "##\n",
        "som = []\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM(map_wth, map_hgt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au4lmJTgd8On"
      },
      "source": [
        "## hyper parameters below\n",
        "## label list below\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()\n",
        "\n",
        "# display neurons weights as mnist digits\n",
        "som_grid = plt.figure(figsize=(10, 10)) # width, height in inches\n",
        "for n in range(map_wth*map_hgt):\n",
        "    ##\n",
        "    ## Must make this into time series plot. Shouldnt be the thing\n",
        "    ##\n",
        "    #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "    sub.set_axis_off()\n",
        "    clr = sub.plot(weights[n]) # imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "    #plt.colorbar(clr)\n",
        "#plt.savefig(\"plots/som_weights.png\")\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWjN57oNMZnc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izaw5vm7N6EW"
      },
      "source": [
        "## Mapa 2 por 2 por 2 = 8 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53d3OR15MaKH"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ####################################################################################################\n",
        "# GPU-based Self-Organizing-Map by Lyes Khacef.\n",
        "# Reference: L. Khacef, V. Gripon, and B. Miramond, “GPU-based self-organizing-maps\n",
        "# for post-labeled few-shot unsupervised learning”, in International Conference On\n",
        "# Neural Information Processing (ICONIP), 2020.\n",
        "# ####################################################################################################\n",
        "\n",
        "# imports\n",
        "#from imports import *\n",
        "#from gpu_check import *\n",
        "#from data_load import *\n",
        "#from tf_ksom import *\n",
        "#from label import *\n",
        "#from test import *\n",
        "\n",
        "# hyper-parameters\n",
        "train_data = 60000\n",
        "label_data = 600\n",
        "test_data = 10000\n",
        "input_dim = 400 #784\n",
        "map_wth = 2\n",
        "map_hgt = 2\n",
        "map_dpt = 2\n",
        "class_nbr = 10\n",
        "nbr_epochs = 40 # 20\n",
        "eps_i_list = [1.0]\n",
        "eps_f_list = [0.01]\n",
        "eta_i_list = [10.0]\n",
        "eta_f_list = [0.01]\n",
        "sigma_kernel = 1.0\n",
        "\n",
        "# GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError(\"GPU device not found!\")\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "##\n",
        "## So, what if theres not test data\n",
        "##\n",
        "# load dataset\n",
        "# x_train, index_train, x_label, index_label, x_test, index_test, label_data = get_dataset(train_data, label_data, test_data)\n",
        "index_train, x_label, index_label, x_test, index_test, label_data = False, False, False, False, False, False\n",
        "x_train = npdf1\n",
        "\n",
        "##\n",
        "## make the ref from outside scope\n",
        "##\n",
        "som = []\n",
        "\n",
        "##\n",
        "## how the hell\n",
        "##\n",
        "# false run_som(false, false, false, eta_f):\n",
        "def run_som(eps_i, eps_f, eta_i, eta_f):\n",
        "    print(\"\\nHyper-parameters:   # eps_i = %f   # eps_f = %f   # eta_i = %f   # eta_f = %f\" % (eps_i, eps_f, eta_i, eta_f))\n",
        "    # train the network\n",
        "    som = KSOM_3D(map_wth, map_hgt, map_dpt, input_dim)\n",
        "    start_time = timeit.default_timer()\n",
        "    som.train(nbr_epochs, eps_i, eps_f, eta_i, eta_f, x_train, x_label, index_label, x_test, index_test)\n",
        "    end_time = timeit.default_timer()\n",
        "    print(\"\\nSOM training time = \", end_time - start_time)\n",
        "    weights = som.get_weights().numpy()\n",
        "\n",
        "    # save the weights\n",
        "    #np.save(\"weights/som_weights.npy\", weights)\n",
        "\n",
        "    ##\n",
        "    ## Now you no label\n",
        "    ##\n",
        "    # label the network\n",
        "    #neuron_label = labeling(label_data, class_nbr, weights, x_label, index_label, sigma_kernel)\n",
        "    neuron_label = False\n",
        "\n",
        "    ##\n",
        "    ## Now you no accuracy\n",
        "    # test the network\n",
        "    #accuracy = test(class_nbr, weights, x_test, index_test, neuron_label, sigma_kernel)\n",
        "    accuracy = False\n",
        "\n",
        "    return weights, accuracy\n",
        "\n",
        "# hyper-parameters grid search\n",
        "hyper_param_list, accuracy_list = [], []\n",
        "for eps_i in eps_i_list:\n",
        "    for eps_f in eps_f_list:\n",
        "        for eta_i in eta_i_list:\n",
        "            for eta_f in eta_f_list:\n",
        "                hyper_param_list.append([eps_i, eps_f, eta_i, eta_f])\n",
        "                weights, accuracy = run_som(eps_i, eps_f, eta_i, eta_f)\n",
        "                accuracy_list.append(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgK04WK4OlOl"
      },
      "source": [
        "# display neurons weights as time series\n",
        "som_grid = plt.figure(figsize=(10, 10)) # width, height in inches\n",
        "for i in range(map_wth):\n",
        "  for j in range(map_hgt):\n",
        "    for k in range(map_dpt):\n",
        "      sub = som_grid.add_subplot(map_wth, map_hgt, 2*j+i+1)\n",
        "      #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "      sub.set_axis_off()\n",
        "      clr = sub.plot(range(400),weights[4*k+2*i+j],k)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3RWKnXtQef_"
      },
      "source": [
        "Certo... analisando rapidamente...\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      1,1, azul eh 0-ply e verde 90-ply\n",
        "    </td>\n",
        "    <td>\n",
        "      1,2, azul eh 0-ply e verde 90-ply\n",
        "    </td>\n",
        "  </tr>\n",
        "\n",
        "  <tr>\n",
        "    <td>\n",
        "      2,1, azul eh +-45-ply e verde 0-ply<br>\n",
        "      ou<br>\n",
        "      2,1, azul eh 90-ply invertido e verrde eh +-45-ply\n",
        "    </td>\n",
        "    <td>\n",
        "      2,2, azul eh 0-ply e verde +-45-ply\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQAp2fXycQrl"
      },
      "source": [
        "# display neurons weights as time series\n",
        "som_grid = plt.figure(figsize=(10, 10)) # width, height in inches\n",
        "for i in range(map_wth):\n",
        "  for j in range(map_hgt):\n",
        "    sub = som_grid.add_subplot(map_wth*map_dpt, map_hgt, 4*k+2*i+j+1,projection='3d')\n",
        "    for k in range(map_dpt):\n",
        "        #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "        sub.set_axis_off()\n",
        "        clr = sub.plot(range(400),weights[4*k+2*i+j],k)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RuFt42GcPdl"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNYi3C9Baqhy"
      },
      "source": [
        "# 4. Unified Distance Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_PuuA66awrC"
      },
      "source": [
        "plt.imshow(umat, cmap=plt.cm.get_cmap('RdYlBu_r'), alpha=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDxNV1DSTtY6"
      },
      "source": [
        "## hyper parameters below\n",
        "## label list below\n",
        "\n",
        "# GPU memory check\n",
        "gpu_report()\n",
        "\n",
        "# display neurons weights as mnist digits\n",
        "som_grid = plt.figure(figsize=(10, 10)) # width, height in inches\n",
        "for n in range(map_wth*map_hgt):\n",
        "    ##\n",
        "    ## Must make this into time series plot. Shouldnt be the thing\n",
        "    ##\n",
        "    #image = weights[n].reshape([28,28]) # x_train[num] is the 784 normalized pixel values\n",
        "    sub = som_grid.add_subplot(map_wth, map_hgt, n + 1)\n",
        "    sub.set_axis_off()\n",
        "    clr = sub.plot(weights[n]) # imshow(image, cmap = plt.get_cmap(\"jet\"), interpolation = \"nearest\")\n",
        "    #plt.colorbar(clr)\n",
        "#plt.savefig(\"plots/som_weights.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeH9yjdK_ZNH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAr9vET5vnhJ"
      },
      "source": [
        "plt.savefig(\"plots/som_weights.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqnjan3e_AxL"
      },
      "source": [
        "\n",
        "# best hyper-parameters\n",
        "best_accuracy = np.max(accuracy_list)\n",
        "best_hyper_param = hyper_param_list[np.argmax(accuracy_list)]\n",
        "print(\"Best accuracy = \", best_accuracy)\n",
        "print(\"Best hyper-parameters:   # eps_i = %f   # eps_f = %f   # sig_i = %f   # sig_f = %f\" % (best_hyper_param[0], best_hyper_param[1], best_hyper_param[2], best_hyper_param[3]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxJKrd6DxI00"
      },
      "source": [
        "label_list = [label_data]\n",
        "for label_data in label_list:\n",
        "    print(\"\\n---------- Labels = %d ----------\" % label_data)\n",
        "    run_labeling(weights, label_data, x_train, index_train, x_test, index_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}